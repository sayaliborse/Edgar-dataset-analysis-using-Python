{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edgar Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import sys\n",
    "import threading\n",
    "import logging\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To parse the HTML page, BeautifulSoup library is used\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read and open zipfile, zipfile module is used\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon S3\n",
    "\n",
    "import boto.s3\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: This function calculates the summary statistics of the columns in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(all_data):\n",
    "    data = pd.DataFrame()\n",
    "    data = all_data\n",
    "    \n",
    "    \n",
    "    # summary = pd.DataFrame()\n",
    "    logging.debug('In the function : summary')\n",
    "    csvpath=str(os.getcwd())\n",
    "    \n",
    "    # add Timestamp for the analysis purpose\n",
    "    data['Timestamp'] = data[['date', 'time']].astype(str).sum(axis=1)\n",
    "    # Create a summary that groups ip by date\n",
    "    \n",
    "    summary1=data['ip'].groupby(data['date']).describe()\n",
    "    summaryipdescribe = pd.DataFrame(summary1)\n",
    "    s=summaryipdescribe.transpose()\n",
    "    s.to_csv(csvpath+\"/summaryipbydatedescribe.csv\")\n",
    "    \n",
    "    # Create a summary that groups cik by accession number\n",
    "    summary2 = data['extention'].groupby(data['cik']).describe()\n",
    "    summarycikdescribe = pd.DataFrame(summary2)\n",
    "    summarycikdescribe.to_csv(csvpath+\"/summarycikbyextentiondescribe.csv\")\n",
    "    \n",
    "    # get Top 10 count of all cik with their accession number\n",
    "    data['COUNT'] = 1  # initially, set that counter to 1.\n",
    "    group_data = data.groupby(['date', 'cik', 'accession'])['COUNT'].count()  # sum function\n",
    "    rankedData=group_data.rank()\n",
    "    summarygroup=pd.DataFrame(rankedData)\n",
    "    summarygroup.to_csv(csvpath+\"/Top10cik.csv\")\n",
    "    \n",
    "    \n",
    "    # For anomaly detection -check the length of cik\n",
    "    data['cik'] = data['cik'].astype('str')\n",
    "    data['cik_length'] = data['cik'].str.len()\n",
    "    data[(data['cik_length'] > 10)]\n",
    "    data['COUNT'] = 1\n",
    "    datagroup=pd.DataFrame(data)\n",
    "    datagroup.to_csv(csvpath+\"/LengthOfCikForAnomalyDetection.csv\")\n",
    "    \n",
    "    \n",
    "    # Per code count\n",
    "    status = data.groupby(['code']).count()  # sum function\n",
    "    status['COUNT']\n",
    "    summary=pd.DataFrame(status)\n",
    "    summary.to_csv(csvpath+\"/PercodeCount.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_missingValues(all_data):\n",
    "    data = pd.DataFrame()\n",
    "    logging.debug('In the function : replace_missingValues')\n",
    "    all_data.loc[all_data['extention'] == '.txt', 'extention'] = all_data[\"accession\"].map(str) + all_data[\"extention\"]\n",
    "    all_data['browser'] = all_data['browser'].fillna('win')\n",
    "    all_data['size'] = all_data['size'].fillna(0)\n",
    "    all_data['size'] = all_data['size'].astype('int64')\n",
    "    all_data = pd.DataFrame(all_data.join(all_data.groupby('cik')['size'].mean(), on='cik', rsuffix='_newsize'))\n",
    "    all_data['size_newsize'] = all_data['size_newsize'].fillna(0)\n",
    "    all_data['size_newsize'] = all_data['size_newsize'].astype('int64')\n",
    "    all_data.loc[all_data['size'] == 0, 'size'] = all_data.size_newsize\n",
    "    del all_data['size_newsize']\n",
    "    data = all_data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: The datatype some columns in the dataframe is converted from floar to int for summary calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dataTypes(all_data):\n",
    "    logging.debug('In the function : change_dataTypes')\n",
    "    all_data['zone'] = all_data['zone'].astype('int64')\n",
    "    all_data['cik'] = all_data['cik'].astype('int64')\n",
    "    all_data['code'] = all_data['code'].astype('int64')\n",
    "    all_data['idx'] = all_data['idx'].astype('int64')\n",
    "    all_data['noagent'] = all_data['noagent'].astype('int64')\n",
    "    all_data['norefer'] = all_data['norefer'].astype('int64')\n",
    "    all_data['crawler'] = all_data['crawler'].astype('int64')\n",
    "    all_data['find'] = all_data['find'].astype('int64')\n",
    "    newdata = replace_missingValues(all_data)\n",
    "    newdata.to_csv(\"merged.csv\",encoding='utf-8')\n",
    "    summary(newdata)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: The below function will create a dataframe of the extracted data from the zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(path):\n",
    "    logging.debug('In the function : create_dataframe')\n",
    "    all_data = pd.DataFrame()\n",
    "    # The below for loop will go through every file in the folder that  has been passed as input\n",
    "    # And whose extention is .csv to create dataframes out of it\n",
    "    for f in glob.glob(path + '/log*.csv'):\n",
    "        df = pd.read_csv(f, parse_dates=[1])\n",
    "        all_data = all_data.append(df, ignore_index=True)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: The below function just makes sure that the path provided as the input exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assure_path_exists(path):\n",
    "    logging.debug('In a function : assure_path_exists')\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: The below function will get the download the zip files links from step 3 and store the data on the local\n",
    "# This function will create a folder with the same name as the year entered by the user\n",
    "# It will then iterate over every element in the list which stored the zip file links of the first day of every month\n",
    "# It will then extract the zip file data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataOnLocal(monthlistdata, year):\n",
    "    logging.debug('In the function : get_dataOnLocal')\n",
    "    df = pd.DataFrame()\n",
    "    # The below code will basically a create a folder of the same name as the year entered by the user\n",
    "    foldername = str(year)\n",
    "    path = str(os.getcwd()) + \"/\" + foldername\n",
    "    # This is step 5 which will check whether this path is valid or not\n",
    "    assure_path_exists(path)\n",
    "    for month in monthlistdata:\n",
    "        # The below code will extract the data from the zip file\n",
    "        with urlopen(month) as zipresp:\n",
    "            with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "                zfile.extractall(path)\n",
    "    df = create_dataframe(path)\n",
    "    # The below function is step 7 which will change the datatypes of the columns in the dataframe\n",
    "    change_dataTypes(df)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: The below function is baiclaly passing the link generated in Step 2 which is the link of a particular year\n",
    "# (cotd) entered by the user. Now this link has all the zip file for all the days in that year\n",
    "# (cotd) But we are interested in only the first day of every month of that particular year\n",
    "#(cotd) So this function will get us a list of the zip file links for the first day of every month for the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_allmonth_data(linkhtml, year):\n",
    "    logging.debug('In the function : get_allmonth_data')\n",
    "    allzipfiles = BeautifulSoup(linkhtml, \"html.parser\")\n",
    "    ziplist = allzipfiles.find_all('li')\n",
    "    monthlistdata = []\n",
    "    count = 0\n",
    "    for li in ziplist:\n",
    "        zipatags = li.findAll('a')\n",
    "        for zipa in zipatags:\n",
    "            if \"01.zip\" in zipa.text:\n",
    "                monthlistdata.append(zipa.get('href'))\n",
    "    get_dataOnLocal(monthlistdata, year)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 : Once you get the year number, you need to go to the main URL and get the url of that particular year\n",
    "# Eg: if the year entered is 2006, the URL generated by this function is \"https://www.sec.gov/files/edgar2006_1.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(year):\n",
    "    logging.debug('In the function : get_url')\n",
    "    url = 'https://www.sec.gov/data/edgar-log-file-data-set.html'\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    all_div = soup.findAll(\"div\", attrs={'id': 'asyncAccordion'})\n",
    "    for div in all_div:\n",
    "        h2tag = div.findAll(\"a\")\n",
    "        for a in h2tag:\n",
    "            if str(year) in a.get('href'):\n",
    "                global ahref\n",
    "                ahref = a.get('href')\n",
    "    linkurl = 'https://www.sec.gov' + ahref\n",
    "    logging.debug('Calling the initial URL')\n",
    "    linkhtml = urlopen(linkurl)\n",
    "    print(linkhtml)\n",
    "    get_allmonth_data(linkhtml, year)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: The below function will check if the year entered by the user is a valid year or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_year(year):\n",
    "    logging.debug('In the function : valid_year')\n",
    "    logYear = ['2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015',\n",
    "               '2016']\n",
    "    for log in logYear:\n",
    "        try:\n",
    "            if year in log:\n",
    "                get_url(year)\n",
    "        except:\n",
    "            print(\"Data for\" + year + \"does not exist\")\n",
    "            \"Data for\" + year + \"does not exist\"\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below code is basically uploading the log file and the zip file to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(AWS_ACCESS_KEY_ID,AWS_SECRET_ACCESS_KEY,inputLocation,filepaths):\n",
    "  \n",
    "    try:\n",
    "        conn = boto.connect_s3(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n",
    "        print(\"Connected to S3\")\n",
    "    except:\n",
    "        logging.info(\"Amazon keys are invalid!!\")\n",
    "        print(\"Amazon keys are invalid!!\")\n",
    "        exit()\n",
    "        \n",
    "    loc=''\n",
    "\n",
    "    if inputLocation == 'APNortheast':\n",
    "        loc=boto.s3.connection.Location.APNortheast\n",
    "    elif inputLocation == 'APSoutheast':\n",
    "        loc=boto.s3.connection.Location.APSoutheast\n",
    "    elif inputLocation == 'APSoutheast2':\n",
    "        loc=boto.s3.connection.Location.APSoutheast2\n",
    "    elif inputLocation == 'CNNorth1':\n",
    "        loc=boto.s3.connection.Location.CNNorth1\n",
    "    elif inputLocation == 'EUCentral1':\n",
    "        loc=boto.s3.connection.Location.EUCentral1\n",
    "    elif inputLocation == 'EU':\n",
    "        loc=boto.s3.connection.Location.EU\n",
    "    elif inputLocation == 'SAEast':\n",
    "        loc=boto.s3.connection.Location.SAEast\n",
    "    elif inputLocation == 'USWest':\n",
    "        loc=boto.s3.connection.Location.USWest\n",
    "    elif inputLocation == 'USWest2':\n",
    "        loc=boto.s3.connection.Location.USWest2\n",
    "    \n",
    "    try:   \n",
    "        ts = time.time()\n",
    "        st = datetime.datetime.fromtimestamp(ts)    \n",
    "        bucket_name = 'adsassignment1part2'+str(st).replace(\" \", \"\").replace(\"-\", \"\").replace(\":\",\"\").replace(\".\",\"\")\n",
    "        bucket = conn.create_bucket(bucket_name, location=loc)\n",
    "        print(\"bucket created\")\n",
    "        s3 = boto3.client('s3',\n",
    "                          aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "                          aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "        \n",
    "        print('s3 client created')\n",
    "        \n",
    "        for f in filepaths:\n",
    "            try:\n",
    "                s3.upload_file(f, bucket_name,os.path.basename(f),\n",
    "                Callback=ProgressPercentage(os.path.basename(f)))\n",
    "                print(\"File successfully uploaded to S3\",f,bucket)\n",
    "            except Exception as detail:\n",
    "                print(detail)\n",
    "                print(\"File not uploaded\")\n",
    "                exit()\n",
    "        \n",
    "    except:\n",
    "        logging.info(\"Amazon keys are invalid!!\")\n",
    "        print(\"Amazon keys are invalid!!\")\n",
    "        exit()\n",
    "\n",
    "#do not forget to use the variable filepaths\n",
    "def zipdir(path, ziph, filepaths):\n",
    "    ziph.write(os.path.join('merged.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressPercentage(object):\n",
    "    def __init__(self, filename):\n",
    "        self._filename = filename\n",
    "        self._size = float(os.path.getsize(filename))\n",
    "        self._seen_so_far = 0\n",
    "        self._lock = threading.Lock()\n",
    "    def __call__(self, bytes_amount):\n",
    "        # To simplify we'll assume this is hooked up\n",
    "        # to a single filename.\n",
    "        with self._lock:\n",
    "            self._seen_so_far += bytes_amount\n",
    "            percentage = (self._seen_so_far / self._size) * 100\n",
    "            sys.stdout.write(\n",
    "                \"\\r%s  %s / %s  (%.2f%%)\" % (\n",
    "                    self._filename, self._seen_so_far, self._size,\n",
    "                    percentage))\n",
    "            sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the Year you want the analysis for\n",
      "Enter year from 2003 to 2017: 2006\n",
      "Year= 2006\n",
      "Access Key= \n",
      "Secret Access Key= \n",
      "Location= \n",
      "log_Edgar_2006_20190823_090701.txt\n",
      "<http.client.HTTPResponse object at 0x000000000931EA20>\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    argLen=len(sys.argv)\n",
    "    year=''\n",
    "    accessKey=''\n",
    "    secretAccessKey=''\n",
    "    inputLocation=''\n",
    "    \n",
    "    print(\"Enter the Year you want the analysis for\")\n",
    "    year = input(\"Enter year from 2003 to 2017: \")\n",
    "\n",
    "    for i in range(1,argLen):\n",
    "        val=sys.argv[i]\n",
    "        if val.startswith('year='):\n",
    "            pos=val.index(\"=\")\n",
    "            year=val[pos+1:len(val)]\n",
    "            continue\n",
    "        elif val.startswith('accessKey='):\n",
    "            pos=val.index(\"=\")\n",
    "            accessKey=val[pos+1:len(val)]\n",
    "            continue\n",
    "        elif val.startswith('secretKey='):\n",
    "            pos=val.index(\"=\")\n",
    "            secretAccessKey=val[pos+1:len(val)]\n",
    "            continue\n",
    "        elif val.startswith('location='):\n",
    "            pos=val.index(\"=\")\n",
    "            inputLocation=val[pos+1:len(val)]\n",
    "            continue\n",
    "\n",
    "    print(\"Year=\",year)\n",
    "    print(\"Access Key=\",accessKey)\n",
    "    print(\"Secret Access Key=\",secretAccessKey)\n",
    "    print(\"Location=\",inputLocation)        \n",
    "        \n",
    "    logfilename = 'log_Edgar_'+ year + '_' + st + '.txt'\n",
    "    print(logfilename)\n",
    "    logging.basicConfig(filename=logfilename, level=logging.DEBUG,\n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logging.debug('Program Start')\n",
    "    logging.debug('*************')    \n",
    "    logging.debug('Calling the initial URL'.format(year))\n",
    "    \n",
    "    #generate files\n",
    "    valid_year(year)\n",
    "    \n",
    "    #prepare log file so that it can be uploaded to cloud\n",
    "    logger = logging.getLogger()\n",
    "    logger.disabled = True\n",
    "    \n",
    "    filepaths = []\n",
    "    filepaths.append(os.path.join(logfilename))\n",
    "    filepaths.append(os.path.join('merged.csv'))\n",
    "    \n",
    "    logging.info('Compiled csv and log file zipped')\n",
    "    \n",
    "    upload_to_s3(accessKey,secretAccessKey,inputLocation,filepaths)\n",
    "    \n",
    "    logger.disabled = False\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
