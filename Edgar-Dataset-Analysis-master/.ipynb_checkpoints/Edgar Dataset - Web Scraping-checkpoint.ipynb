{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edgar Dataset - Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import datetime\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import boto3\n",
    "import threading\n",
    "import boto.s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the correct URL by taking the CIK and Document Accession Number as inputs from the users. \n",
    "# This will give us the main link (index page) of the particular company\n",
    "# We need to scrape data from the 10Q Files and so we need to generate a final link which directly goes to 10Q files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(cik, accession):\n",
    "    logging.debug('In the function : get_url')\n",
    "    cik = str(cik)\n",
    "    accession = str(accession)\n",
    "    cik = cik.lstrip('0')\n",
    "    acc = re.sub(r'[-]', r'', accession)\n",
    "    url = 'https://www.sec.gov/Archives/edgar/data/' + cik + '/' + acc + '/' + accession + '/-index.htm'\n",
    "    logging.debug('Calling the initial URL for CIK {} & Accession Number {} to open URL {}'.format(cik, acc, url))\n",
    "    try:\n",
    "        page_open = urllib.request.urlopen(url)\n",
    "        if page_open.code == 200:\n",
    "            logging.debug(\"URL Exisits\")\n",
    "            return get_final_url(url)\n",
    "    except:\n",
    "        logging.debug(\"Invalid URL. Please re-validate\".format(url))\n",
    "        print(\"Invalid URL. Please re-validate\".format(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below code will give us the link to the 10Q data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_url(url):\n",
    "    final_url = \"\"\n",
    "    logging.debug('In the function : get_final_url')\n",
    "    html = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    all_tables = soup.find('table', class_='tableFile')\n",
    "    tr = all_tables.find_all('tr')\n",
    "    for row in tr:\n",
    "        final_url = row.findNext(\"a\").attrs['href']\n",
    "        break\n",
    "    next_url = \"https://www.sec.gov\" + final_url\n",
    "    logging.debug(\"Final URL {}:\".format(next_url))\n",
    "    print(next_url)\n",
    "    return get_soup(next_url)\n",
    "    #return (next_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we have the link, we need to use beautiful soup to retreive the html code of the page\n",
    "# And then find all the elements on this web page with the \"table\" tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    try:\n",
    "        logging.debug('In the function : get_soup')\n",
    "        htmlpage = urllib.request.urlopen(url)\n",
    "        page = BeautifulSoup(htmlpage, \"html.parser\") # This returns the entire code of the html page whose link has been passed as input\n",
    "        return find_all_tables(page)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below function will return all the table tags from the html page so that we find the 10Q files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_tables(page):\n",
    "    logging.debug('In the function : find_all_tables')\n",
    "    all_divtables = page.find_all('table') # This basically returns a list of all table tags in the page\n",
    "    find_all_datatables(page, all_divtables)\n",
    "    return foldername(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once, the tables are found, we need to create a folder so that we can download all the files into that folder. \n",
    "# The below code is to generate a folder name from the URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foldername(page):\n",
    "    title = page.find('filename').contents[0]\n",
    "    if \".htm\" in title:\n",
    "        foldername = title.split(\".htm\")\n",
    "        logging.debug('In the function : foldername{}'.format(foldername[0]))\n",
    "        return foldername[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below function is writing csv files a zip folder under the folder that has been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_dir(path_dir, path_file_zip=''):\n",
    "    print(os.path.dirname(path_dir))\n",
    "    if not path_file_zip:\n",
    "        logging.debug('In a function : zip_dir')\n",
    "        path_file_zip = os.path.join(\n",
    "            os.path.dirname(path_dir), os.path.basename(path_dir) + '.zip')\n",
    "    with zipfile.ZipFile(path_file_zip, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n",
    "        for root, dirs, files in os.walk(path_dir):\n",
    "            for file_or_dir in files + dirs:\n",
    "                \n",
    "                zip_file.write(\n",
    "                    os.path.join(root, file_or_dir),\n",
    "                    os.path.relpath(os.path.join(root, file_or_dir),\n",
    "                                    os.path.join(path_dir, os.path.pardir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below function is to make sure that the path given to create a folder actually exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assure_path_exists(path):\n",
    "    logging.debug('In a function : assure_path_exists')\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is basically to check whether the <tr> tags under the <table> tags have any color\n",
    "# So if you visit the webpage, you will see that the 10Q data is in the tables with blue and white rows\n",
    "# So we are trying to filter those tables from all the tables based on their background color attribute\n",
    "# So if any of the row tag under the table tag has backgrounmd color, this means that it is our data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checktag(param):\n",
    "    setflag = \"false\"\n",
    "    datatabletags = [\"background\", \"bgcolor\", \"background-color\"]\n",
    "    for x in datatabletags:\n",
    "        if x in param:\n",
    "            setflag = \"true\"\n",
    "    return setflag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkheadertag(param):\n",
    "    logging.debug('In a function : checkheadertag')\n",
    "    setflag=\"false\"\n",
    "    datatabletags=[\"center\",\"bold\"]\n",
    "    for x in datatabletags:\n",
    "        if x in param:\n",
    "            setflag=\"true\"\n",
    "    return setflag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printtable(table):\n",
    "    logging.debug('In a function : printtable')\n",
    "    printtable = []\n",
    "    printtrs = table.find_all('tr')\n",
    "    for tr in printtrs:\n",
    "        data=[]\n",
    "        pdata=[]\n",
    "        printtds=tr.find_all('td')\n",
    "        for elem in printtds:\n",
    "            x=elem.text;\n",
    "            x=re.sub(r\"['()]\",\"\",str(x))\n",
    "            x=re.sub(r\"[$]\",\" \",str(x))\n",
    "            if(len(x)>1):\n",
    "                x=re.sub(r\"[â€”]\",\"\",str(x))\n",
    "                pdata.append(x)\n",
    "        data=([elem.encode('utf-8') for elem in pdata])\n",
    "        printtable.append([elem.decode('utf-8').strip() for elem in data])\n",
    "    return printtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The find_all_tables function is used tofind all the tables form the page\n",
    "# But we need only the data tables which is found by the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_datatables(page, all_divtables):\n",
    "    logging.debug('In a function : find_all_datatables')\n",
    "    count = 0\n",
    "    allheaders=[]\n",
    "    # Going through all the tables one table at a time to find our data tables\n",
    "    for table in all_divtables:\n",
    "        # The bluetables will basically store the list of tables with blue backgroud rows\n",
    "        bluetables = []\n",
    "        trs = table.find_all('tr')\n",
    "        # Going into each row of one table at a time\n",
    "        for tr in trs:\n",
    "            global flagtr\n",
    "            # If the style attribute of row tag contains background color, enter that particular table\n",
    "            # (cotd) as an input to the \"printtable\" function\n",
    "            if checktag(str(tr.get('style'))) == \"true\" or checktag(str(tr)) == \"true\":\n",
    "                logging.debug('Checking data tables at Row Level')\n",
    "                bluetables = printtable(tr.find_parent('table'))\n",
    "                break\n",
    "            else:\n",
    "                tds = tr.find_all('td')\n",
    "                for td in tds:\n",
    "                    if checktag(str(td.get('style'))) == \"true\" or checktag(str(td)) == \"true\":\n",
    "                        logging.debug('Checking data tables at Column Level')\n",
    "                        bluetables = printtable(td.find_parent('table'))\n",
    "                        break\n",
    "            if not len(bluetables) == 0:\n",
    "                break\n",
    "        if not len(bluetables) == 0:\n",
    "            logging.debug('Total Number of data tables to be created {}'.format(len(bluetables)))\n",
    "            count += 1\n",
    "            # The below code is basically trying to find the title of each of the coloured data table\n",
    "            # (cotd) and putting the titles in \"allheaders\" list declared above\n",
    "            ptag=table.find_previous('p');\n",
    "            while ptag is not None and checkheadertag(ptag.get('style'))==\"false\" and len(ptag.text)<=1:\n",
    "                ptag=ptag.find_previous('p')\n",
    "                if checkheadertag(ptag.get('style'))==\"true\" and len(ptag.text)>=2:\n",
    "                    global name\n",
    "                    name=re.sub(r\"[^A-Za-z0-9]+\",\"\",ptag.text)\n",
    "                    if name in allheaders:\n",
    "                        hrcount+=1\n",
    "                        hrname=name+\"_\"+str(hrcount)\n",
    "                        allheaders.append(hrname)\n",
    "                    else:\n",
    "                        hrname=name\n",
    "                        allheaders.append(hrname)\n",
    "                        break\n",
    "            # Folder name where a zip file would be created with all the data tables as csv files in it            \n",
    "            folder_name = foldername(page)\n",
    "            logging.debug('folder created with folder Name{}'.format(folder_name))\n",
    "            # Path to create the folder\n",
    "            path = str(os.getcwd()) + \"/\" + folder_name\n",
    "            logging.debug('Path for csv creation {}'.format(path))\n",
    "            # Checking whether the path exists by calling the assure_path_exists function\n",
    "            assure_path_exists(path)\n",
    "            if(len(allheaders)==0):\n",
    "                filename=folder_name+\"-\"+str(count)\n",
    "            else:\n",
    "                filename=allheaders.pop()\n",
    "            # csvname variable is basically creating unique csv names for the csv files \n",
    "            # From the title names of each of the table\n",
    "            # That is why the above code is popping each element from the allheaders list which has title names\n",
    "            csvname=filename+\".csv\"\n",
    "            logging.debug('file creation Name{}'.format(csvname))\n",
    "            csvpath = path + \"/\" + csvname\n",
    "            os.path.abspath(csvpath)\n",
    "            print(csvpath)\n",
    "            logging.debug('CSV Path for the file creation {}'.format(csvpath))\n",
    "            with open(csvpath, 'w', encoding='utf-8-sig', newline='') as f:\n",
    "                # Writing a csv file\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerows(bluetables)\n",
    "            zip_dir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below code is basically uploading the log file and the zip file to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(AWS_ACCESS_KEY_ID,AWS_SECRET_ACCESS_KEY,inputLocation,filepaths):\n",
    "  \n",
    "    try:\n",
    "        conn = boto.connect_s3(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n",
    "        print(\"Connected to S3\")\n",
    "    except:\n",
    "        logging.info(\"Amazon keys are invalid!!\")\n",
    "        print(\"Amazon keys are invalid!!\")\n",
    "        exit()\n",
    "        \n",
    "    loc=''\n",
    "\n",
    "    if inputLocation == 'APNortheast':\n",
    "        loc=boto.s3.connection.Location.APNortheast\n",
    "    elif inputLocation == 'APSoutheast':\n",
    "        loc=boto.s3.connection.Location.APSoutheast\n",
    "    elif inputLocation == 'APSoutheast2':\n",
    "        loc=boto.s3.connection.Location.APSoutheast2\n",
    "    elif inputLocation == 'CNNorth1':\n",
    "        loc=boto.s3.connection.Location.CNNorth1\n",
    "    elif inputLocation == 'EUCentral1':\n",
    "        loc=boto.s3.connection.Location.EUCentral1\n",
    "    elif inputLocation == 'EU':\n",
    "        loc=boto.s3.connection.Location.EU\n",
    "    elif inputLocation == 'SAEast':\n",
    "        loc=boto.s3.connection.Location.SAEast\n",
    "    elif inputLocation == 'USWest':\n",
    "        loc=boto.s3.connection.Location.USWest\n",
    "    elif inputLocation == 'USWest2':\n",
    "        loc=boto.s3.connection.Location.USWest2\n",
    "    \n",
    "    try:   \n",
    "        ts = time.time()\n",
    "        st = datetime.datetime.fromtimestamp(ts)    \n",
    "        bucket_name = 'adsassignment1part1'+str(st).replace(\" \", \"\").replace(\"-\", \"\").replace(\":\",\"\").replace(\".\",\"\")\n",
    "        bucket = conn.create_bucket(bucket_name, location=loc)\n",
    "        print(\"bucket created\")        \n",
    "        s3 = boto3.client('s3',\n",
    "                          aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "                          aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "        \n",
    "        print('s3 client created')\n",
    "        \n",
    "        for f in filepaths:\n",
    "            try:\n",
    "                s3.upload_file(f, bucket_name,os.path.basename(f),\n",
    "                Callback=ProgressPercentage(os.path.basename(f)))\n",
    "                print(\"File successfully uploaded to S3\",f,bucket)\n",
    "            except Exception as detail:\n",
    "                print(detail)\n",
    "                print(\"File not uploaded\")\n",
    "                exit()\n",
    "        \n",
    "    except:\n",
    "        logging.info(\"Amazon keys are invalid!!\")\n",
    "        print(\"Amazon keys are invalid!!\")\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressPercentage(object):\n",
    "    def __init__(self, filename):\n",
    "        self._filename = filename\n",
    "        self._size = float(os.path.getsize(filename))\n",
    "        self._seen_so_far = 0\n",
    "        self._lock = threading.Lock()\n",
    "    def __call__(self, bytes_amount):\n",
    "        # To simplify we'll assume this is hooked up\n",
    "        # to a single filename.\n",
    "        with self._lock:\n",
    "            self._seen_so_far += bytes_amount\n",
    "            percentage = (self._seen_so_far / self._size) * 100\n",
    "            sys.stdout.write(\n",
    "                \"\\r%s  %s / %s  (%.2f%%)\" % (\n",
    "                    self._filename, self._seen_so_far, self._size,\n",
    "                    percentage))\n",
    "            sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y%m%d_%H%M%S')\n",
    "    argLen=len(sys.argv)\n",
    "    cik =''\n",
    "    accessionNumber=''\n",
    "    accessKey=''\n",
    "    secretAccessKey=''\n",
    "    inputLocation=''\n",
    "  \n",
    "    for i in range(1,argLen):\n",
    "        val=sys.argv[i]\n",
    "        if val.startswith('cik='):\n",
    "            pos=val.index(\"=\")\n",
    "            cik=val[pos+1:len(val)]\n",
    "            continue\n",
    "        elif val.startswith('accessionNumber='):\n",
    "            pos=val.index(\"=\")\n",
    "            accessionNumber=val[pos+1:len(val)]\n",
    "            continue\n",
    "        elif val.startswith('accessKey='):\n",
    "            pos=val.index(\"=\")\n",
    "            accessKey=val[pos+1:len(val)]\n",
    "            continue\n",
    "        elif val.startswith('secretKey='):\n",
    "            pos=val.index(\"=\")\n",
    "            secretAccessKey=val[pos+1:len(val)]\n",
    "            continue\n",
    "        elif val.startswith('location='):\n",
    "            pos=val.index(\"=\")\n",
    "            inputLocation=val[pos+1:len(val)]\n",
    "            continue\n",
    "    \n",
    "    print(\"CIK=\",cik)\n",
    "    print(\"Accession Number=\",accessionNumber)\n",
    "    print(\"Access Key=\",accessKey)\n",
    "    print(\"Secret Access Key=\",secretAccessKey)\n",
    "    print(\"Location=\",inputLocation)\n",
    "          \n",
    "    logfilename = 'log_Edgar_'+ cik + '_' + st + '.txt' \n",
    "    logging.basicConfig(filename=logfilename, level=logging.DEBUG,format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logging.debug('Program Start')\n",
    "    logging.debug('*************')\n",
    "    logging.debug('Calling the initial URL with CIK Number {} and Accession number {}'.format(cik, accessionNumber))\n",
    "    nameOfFolder=get_url(cik, accessionNumber)\n",
    "    \n",
    "    filepaths = []\n",
    "    filepaths.append(os.path.join(logfilename))\n",
    "    filepaths.append(os.path.join(nameOfFolder + '.zip'))\n",
    "    \n",
    "    logging.info('Compiled csv and log file zipped')\n",
    "    \n",
    "    upload_to_s3(accessKey,secretAccessKey,inputLocation,filepaths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
